<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="https://yuxuan1206.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PanopticRecon</title>
  <link rel="icon" type="image/x-icon" href="static/images/nfatlas.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PanopticRecon: Leverage Open-vocabulary Instance Segmentation for Zero-shot Panoptic Reconstruction</h1>
            <h1 class="is-size-4 publication-authors">IROS 2024 (Oral)</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"> 
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=4Ry3CKsAAAAJ" target="_blank">Xuan Yu</a>,</span>
                <span class="author-block"> 
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=pBEZ7V4AAAAJ" target="_blank">Yili Liu</a>,</span>
                  <span class="author-block">
                    Chenrui Han</a>,</span>
                    <span class="author-block">
                      Sitong Mao</a>,</span>
                        <span class="author-block">
                          Shunbo Zhou</a>,</span>
                            <span class="author-block">
                              <a href="https://mypage.zju.edu.cn/rongxiong" target="_blank">Rong Xiong</a>,</span>
                                  <span class="author-block">
                                    <a href="https://yiyiliao.github.io/" target="_blank">Yiyi Liao</a>,</span>
                                        <span class="author-block">
                                          <a href="https://ywang-zju.github.io/" target="_blank">Yue Wang</a><sup>*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Zhejiang University, Huawei Cloud Computing Technologies Co., Ltd.<!-- <br>Conferance name and year</span> -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2407.01349" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://www.bilibili.com/video/BV1NE421N7Qy/?spm_id_from=333.337.search-card.all.click&vd_source=29b4f2244b05910dcebd67ee967d0ff8" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-video"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/yuxuan1206/PanopticRecon" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code(coming soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
<!--                 <span class="link-block">
                  <a href="https://arxiv.org/abs/2304.04624" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/pre.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
<!--         Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.  -->
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Panoptic reconstruction is a challenging task in 3D scene understanding. However, most existing methods heavily rely on pre-trained semantic segmentation models and known 3D object bounding boxes for 3D panoptic segmentation, which is not available for in-the-wild scenes. In this paper, we propose a novel zero-shot panoptic reconstruction method from RGB-D images of scenes. For zero-shot segmentation, we leverage open-vocabulary instance segmentation, but it has to face partial labeling and instance association challenges. We tackle both challenges by propagating partial labels with the aid of dense generalized features and building a 3D instance graph for associating 2D instance IDs. Specifically, we exploit partial labels to learn a classifier for generalized semantic features to provide complete labels for scenes with dense distilled features. Moreover, we formulate instance association as a 3D instance graph segmentation problem, allowing us to fully utilize the scene geometry prior and all 2D instance masks to infer global unique pseudo 3D instance ID. Our method outperforms state-of-the-art methods on the indoor dataset ScanNet V2 and the outdoor dataset KITTI-360, demonstrating the effectiveness of our graph segmentation method and reconstruction network.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section">
  <div class="container is-max-desktop">
  <div class="columns is-centered">
  <h2 class="title is-3">Approach Overview</h2>
  <p></p>
  </div></div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <img src="./static/images/pipeline.jpg" alt="Teaser" style="width:96%;">
          <!-- <h3 class="title is-5 has-text-centered">Tangent Space Consistency</h3> -->
          PanopticRecon consists of a reconstruction task and a segmentation task. The first step of the reconstruction task realizes the implicit surface reconstruction through RGB-D observations to provide the scene geometry for the segmentation task. Secondly, the segmentation task builds a graph from the normal of mesh, and infers 3D pseudo IDs to associate the 2D instance IDs by instance mask of Grounded SAM. In addition, 3D instance ID corrects some of the erroneous semantic labels. Then, the second reconstruction step realizes 2D-3D labeling supervised by consistent semantic and instance labels, and finally obtains the panoptic mesh, point cloud, and novel view images of the scene.        </div>
      </div>
      <!--/ TSC. -->

      <!-- <div class="column">
        <div class="columns is-centered">
          <div class="column content">
          <img src="./static/images/neural_sdf_optimization.png" alt="Teaser" style="width:100%;">
            <h3 class="title is-5 has-text-centered">Neural SDF Optimization</h3>
              We project the surface point, found by sphere tracing, onto all views, and enforce the surface normal (i.e., SDF gradient) to be perpendicular to all tangents from visible views. We train the neural SDF with this TSC loss, silhouette, and Eikonal loss.
          </div>

        </div>
      </div> -->
    </div>
    <!--/ Neural SDF Optimization. -->
</section>  

<section class="section">
  <div class="container is-max-desktop">
  <div class="columns is-centered">
  <!-- <h2 class="title is-3">Approach Overview</h2> -->
  <p></p>
  </div></div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <img src="./static/images/graph.jpg" alt="Teaser" style="width:50%;">
          <h3 class="title is-5 has-text-centered">Graph Inference</h3>
          The points in the upper graph in \textbf{(a)} are the nodes (superpoints) of the graph. The color region corresponding to each node is the superface. We determine the nodes in an instance mask of a frame based on the overlap between the instance mask provided by Grounded SAM and the mask projected by the superface, and vote for the edges between the selected nodes. Similarly, we reduce the votes for the edges between nodes corresponding to masks of different instances in that frame. The edges with non-positive votes are finally cut and the nodes connected form an instance as shown in \textbf{(b)}. Once the 3D instance pseudo IDs are obtained, we associate 2D instance IDs while correcting incorrect semantic labels.      </div>
      <!--/ TSC. -->

      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
          <img src="./static/images/network.jpg" alt="Teaser" style="width:50%;">
            <h3 class="title is-5 has-text-centered">Network</h3>
            Based on the reconstruction architecture of a multi-level hash representation grid and small MLP, multi-branch joint optimization achieves multiple tasks of SDF reconstruction, appearance reconstruction, feature distillation, semantic segmentation, and instance segmentation. The loss function of the optimization process is not only the basic loss of SDF and color, but we also set the DINO-v2 feature vector L2-loss to unify the mesh features of similar objects or backgrounds and learn small classifiers targeting the mesh features through the cross-entropy loss of semantics and instances, to achieve the unsupervised situation of some regions due to blank labels to a certain extent.          </div>

        </div>
      </div>
    </div>
    <!--/ Neural SDF Optimization. -->
</section>  


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            Youtube embed code here -->
<!--             <iframe src="https://www.bilibili.com/video/BV1uc411p7qD/?share_source=copy_web&vd_source=695a5f9daaf4836e3e3be61b21be2b0f" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div> -->
<!-- </section> --> 
<!-- End youtube video


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
<!--             <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
<!--             <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
<!--             <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{yu2024panopticrecon,
          title={PanopticRecon: Leverage Open-vocabulary Instance Segmentation for Zero-shot Panoptic Reconstruction},
          author={Yu, Xuan and Liu, Yili and Han, Chenrui and Mao, Sitong and Zhou, Shunbo and Xiong, Rong and Liao, Yiyi and Wang, Yue},
          journal={arXiv preprint arXiv:2407.01349},
          year={2024}
          }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.  
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
